{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../input/iterative-stratification-master')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import json\n",
    "import math\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import torchvision\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.prune\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def random_init(m, init_func=torch.nn.init.xavier_uniform_):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv1d) or isinstance(m, nn.ConvTranspose1d):\n",
    "        init_func(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_hidden, hidden, activation=nn.ReLU, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.linear1 = nn.Sequential(\n",
    "            nn.utils.weight_norm(nn.Linear(in_hidden, hidden)),\n",
    "        )\n",
    "        self.linear2 = nn.Sequential(\n",
    "            nn.BatchNorm1d(hidden),\n",
    "            nn.utils.weight_norm(nn.Linear(hidden, in_hidden)),\n",
    "        )\n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.utils.weight_norm(nn.Linear(in_hidden, hidden)),\n",
    "            nn.BatchNorm1d(in_hidden)\n",
    "        )\n",
    "        self.linear1.apply(random_init).to(DEVICE)\n",
    "        self.linear2.apply(random_init).to(DEVICE)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.linear1(x))\n",
    "        out = self.linear2(out)\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    \n",
    "\n",
    "def seed_everything(seed=1903):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "class MoADataset:\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "        }\n",
    "        # dct['x'] += dct['x'] + (0.1**0.5)*torch.randn(dct['x'].shape)\n",
    "        return dct\n",
    "    \n",
    "class TestDataset:\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
    "        }\n",
    "        return dct\n",
    "    \n",
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device, l1_reg=0):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    \n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = data['x'].to(device)\n",
    "        #print(inputs.shape)\n",
    "        outputs, kl = model(inputs)\n",
    "        kl_div = torch.mean(kl)\n",
    "\n",
    "        loss_recon = loss_fn(outputs, inputs)\n",
    "        print(loss_recon)\n",
    "        print(kl_div)\n",
    "        loss = loss_recon + kl_div\n",
    "        #regularization_loss = 0\n",
    "        #for param in model.parameters():\n",
    "        #    regularization_loss += torch.sum(torch.abs(param))\n",
    "        #loss += l1_reg * regularization_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        final_loss += loss.item()\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    \n",
    "    return final_loss\n",
    "\n",
    "\n",
    "def valid_fn(model, loss_fn, dataloader, device):\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    valid_preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs = data['x'].to(device)\n",
    "        outputs, _ = model(inputs)\n",
    "        loss = loss_fn(outputs, inputs)\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    valid_preds = np.concatenate(valid_preds)\n",
    "    \n",
    "\n",
    "    return final_loss, valid_preds\n",
    "\n",
    "\n",
    "def inference_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs = data['x'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "        \n",
    "        preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    preds = np.concatenate(preds)\n",
    "    \n",
    "    return preds\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features, num_targets, hidden_size, n_res=1):\n",
    "        super(Model, self).__init__()        \n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size).apply(random_init))\n",
    "\n",
    "        self.batch_norm2 = nn.ModuleList()\n",
    "        self.dropout2 = nn.ModuleList()\n",
    "        self.dense2 = nn.ModuleList()\n",
    "\n",
    "        for res in range(n_res):\n",
    "            self.batch_norm2.append(nn.BatchNorm1d(hidden_size).to(DEVICE))\n",
    "            self.dropout2.append(nn.Dropout(0.2).to(DEVICE))\n",
    "            self.dense2.append(ResBlock(hidden_size, hidden_size, device=DEVICE).apply(random_init))\n",
    "                \n",
    "        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout3 = nn.Dropout(0.25)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets).apply(random_init))\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "                \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.activation(self.dense1(x))\n",
    "        for batch_norm, dropout, dense in zip(self.batch_norm2,\n",
    "                                              self.dropout2, \n",
    "                                              self.dense2):\n",
    "            x = batch_norm(x)\n",
    "            x = dropout(x)\n",
    "            x = dense(x)\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "def prune_model(model):\n",
    "    parameters_to_prune = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            module.weight = torch.nn.Parameter(module.weight)\n",
    "            parameters_to_prune += [[module, 'weight']]\n",
    "    torch.nn.utils.prune.global_unstructured(\n",
    "        parameters_to_prune,\n",
    "        pruning_method=torch.nn.utils.prune.L1Unstructured,\n",
    "        amount=0.1,\n",
    "    )\n",
    "    \n",
    "    \n",
    "def process_data(data):\n",
    "    \n",
    "    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n",
    "    \n",
    "    return data\n",
    "\n",
    "seed_everything(seed=1903)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Stochastic(nn.Module):\n",
    "    \"\"\"\n",
    "    Base stochastic layer that uses the\n",
    "    reparametrization trick [Kingma 2013]\n",
    "    to draw a sample from a distribution\n",
    "    parametrised by mu and log_var.\n",
    "    \"\"\"\n",
    "    def reparametrize(self, mu, log_var):\n",
    "        epsilon = Variable(torch.randn(mu.size()), requires_grad=False)\n",
    "\n",
    "        if mu.is_cuda:\n",
    "            epsilon = epsilon.cuda()\n",
    "\n",
    "        # log_std = 0.5 * log_var\n",
    "        # std = exp(log_std)\n",
    "        std = log_var.mul(0.5).exp_()\n",
    "\n",
    "        # z = std * epsilon + mu\n",
    "        z = mu.addcmul(std, epsilon)\n",
    "\n",
    "        return z\n",
    "\n",
    "class GaussianSample(Stochastic):\n",
    "    \"\"\"\n",
    "    Layer that represents a sample from a\n",
    "    Gaussian distribution.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GaussianSample, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.mu = nn.Linear(in_features, out_features)\n",
    "        self.log_var = nn.Linear(in_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = self.mu(x)\n",
    "        log_var = F.softplus(self.log_var(x))\n",
    "\n",
    "        return self.reparametrize(mu, log_var), mu, log_var\n",
    "\n",
    "\n",
    "def log_standard_gaussian(x):\n",
    "    \"\"\"\n",
    "    Evaluates the log pdf of a standard normal distribution at x.\n",
    "    :param x: point to evaluate\n",
    "    :return: log N(x|0,I)\n",
    "    \"\"\"\n",
    "    return torch.sum(-0.5 * math.log(2 * math.pi) - x ** 2 / 2, dim=-1)\n",
    "\n",
    "\n",
    "def log_gaussian(x, mu, log_var):\n",
    "    \"\"\"\n",
    "    Returns the log pdf of a normal distribution parametrised\n",
    "    by mu and log_var evaluated at x.\n",
    "    :param x: point to evaluate\n",
    "    :param mu: mean of distribution\n",
    "    :param log_var: log variance of distribution\n",
    "    :return: log N(x|µ,σ)\n",
    "    \"\"\"\n",
    "    log_pdf = - 0.5 * torch.log(2 * torch.tensor(math.pi, requires_grad=True)) - log_var / 2 - (x - mu)**2 / (2 * torch.exp(log_var))\n",
    "    return torch.sum(log_pdf, dim=-1)\n",
    "    \n",
    "\n",
    "class PlanarNormalizingFlow(nn.Module):\n",
    "    \"\"\"\n",
    "    Planar normalizing flow [Rezende & Mohamed 2015].\n",
    "    Provides a tighter bound on the ELBO by giving more expressive\n",
    "    power to the approximate distribution, such as by introducing\n",
    "    covariance between terms.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features):\n",
    "        super(PlanarNormalizingFlow, self).__init__()\n",
    "        self.u = nn.Parameter(torch.randn(in_features))\n",
    "        self.w = nn.Parameter(torch.randn(in_features))\n",
    "        self.b = nn.Parameter(torch.ones(1))\n",
    "\n",
    "    def forward(self, z):\n",
    "        # Create uhat such that it is parallel to w\n",
    "        uw = torch.dot(self.u, self.w)\n",
    "        muw = -1 + F.softplus(uw)\n",
    "        uhat = self.u + (muw - uw) * torch.transpose(self.w, 0, -1) / torch.sum(self.w ** 2)\n",
    "\n",
    "        # Equation 21 - Transform z\n",
    "        zwb = torch.mv(z, self.w) + self.b\n",
    "\n",
    "        f_z = z + (uhat.view(1, -1) * torch.tanh(zwb).view(-1, 1))\n",
    "\n",
    "        # Compute the Jacobian using the fact that\n",
    "        # tanh(x) dx = 1 - tanh(x)**2\n",
    "        psi = (1 - torch.tanh(zwb)**2).view(-1, 1) * self.w.view(1, -1)\n",
    "        psi_u = torch.mv(psi, uhat)\n",
    "\n",
    "        # Return the transformed output along\n",
    "        # with log determninant of J\n",
    "        logdet_jacobian = torch.log(torch.abs(1 + psi_u) + 1e-8)\n",
    "\n",
    "        return f_z, logdet_jacobian\n",
    "\n",
    "\n",
    "class HFlow(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HFlow, self).__init__()\n",
    "\n",
    "    def forward(self, v, z):\n",
    "        '''\n",
    "        :param v: batch_size (B) x latent_size (L)\n",
    "        :param z: batch_size (B) x latent_size (L)\n",
    "        :return: z_new = z - 2* v v_T / norm(v,2) * z\n",
    "        '''\n",
    "        # v * v_T\n",
    "        vvT = torch.bmm(v.unsqueeze(2), v.unsqueeze(1) )  # v * v_T : batch_dot( B x L x 1 * B x 1 x L ) = B x L x L\n",
    "        # v * v_T * z\n",
    "        vvTz = torch.bmm(vvT, z.unsqueeze(2) ).squeeze(2) # A * z : batchdot( B x L x L * B x L x 1 ).squeeze(2) = (B x L x 1).squeeze(2) = B x L\n",
    "        # calculate norm ||v||^2\n",
    "        norm_sq = torch.sum(v * v, 1) # calculate norm-2 for each row : B x 1\n",
    "        norm_sq = norm_sq.expand(v.size(1), norm_sq.size(0) ) # expand sizes : B x L\n",
    "        # calculate new z\n",
    "        z_new = z - 2 * vvTz / norm_sq.transpose(1, 0) # z - 2 * v * v_T  * z / norm2(v)\n",
    "        return z_new\n",
    "\n",
    "\n",
    "class linIAF(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "    def forward(self, l, z):\n",
    "        '''\n",
    "        :param L: batch_size (B) x latent_size^2 (L^2)\n",
    "        :param z: batch_size (B) x latent_size (L)\n",
    "        :return: z_new = L*z\n",
    "        '''\n",
    "        # L->tril(L)\n",
    "        l_matrix = l.view(-1, self.z_dim, self.z_dim)  # resize to get B x L x L\n",
    "        lt_mask = torch.tril(torch.ones(self.z_dim, self.z_dim), -1)  # lower-triangular mask matrix (1s in lower triangular part)\n",
    "        I = Variable(torch.eye(self.z_dim, self.z_dim).expand(l_matrix.size(0), self.z_dim, self.z_dim))\n",
    "        if self.cuda:\n",
    "            lt_mask = lt_mask.cuda()\n",
    "            I = I.cuda()\n",
    "        lt_mask = Variable(lt_mask)\n",
    "        lt_mask = lt_mask.unsqueeze(0).expand(l_matrix.size(0), self.z_dim, self.z_dim)  # 1 x L x L -> B x L x L\n",
    "        lt = torch.mul(l_matrix, lt_mask) + I  # here we get a batch of lower-triangular matrices with ones on diagonal\n",
    "\n",
    "        # z_new = L * z\n",
    "        z_new = torch.bmm(lt, z.unsqueeze(2)).squeeze(2)  # B x L x L * B x L x 1 -> B x L\n",
    "\n",
    "        return z_new\n",
    "\n",
    "\n",
    "class CombinationL(nn.Module):\n",
    "    def __init__(self, z_dim, n_combination):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.n_combination = n_combination\n",
    "\n",
    "    def forward(self, l, y):\n",
    "        '''\n",
    "        :param l: batch_size (B) x latent_size^2 * n_combination (L^2 * C)\n",
    "        :param y: batch_size (B) x n_combination (C)\n",
    "        :return: l_combination = y * L\n",
    "        '''\n",
    "        # calculate combination of Ls\n",
    "        l_tensor = l.view(-1, self.z_dim ** 2, self.n_combination)  # resize to get B x L^2 x C\n",
    "        y = y.unsqueeze(1).expand(y.size(0), self.z_dim ** 2, y.size(1))  # expand to get B x L^2 x C\n",
    "        l_combination = torch.sum(l_tensor * y, 2).squeeze()\n",
    "        return l_combination\n",
    "\n",
    "\n",
    "class NormalizingFlows(nn.Module):\n",
    "    \"\"\"\n",
    "    Presents a sequence of normalizing flows as a torch.nn.Module.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, n_flows=1, h_last_dim=None, flow_type=PlanarNormalizingFlow):\n",
    "        self.h_last_dim = h_last_dim\n",
    "        self.flows = []\n",
    "        self.flows_a = []\n",
    "        self.n_flows = n_flows\n",
    "        self.flow_type = \"nf\"\n",
    "        for i, features in enumerate(reversed(in_features)):\n",
    "            self.flows += [nn.ModuleList([flow_type(features).cuda() for _ in range(n_flows)])]\n",
    "\n",
    "        super(NormalizingFlows, self).__init__()\n",
    "\n",
    "    def forward(self, z, i=0):\n",
    "        log_det_jacobian = []\n",
    "        flows = self.flows\n",
    "        for flow in flows[i]:\n",
    "            z, j = flow(z)\n",
    "            log_det_jacobian.append(j)\n",
    "        return z, sum(log_det_jacobian)\n",
    "\n",
    "\n",
    "class HouseholderFlow(nn.Module):\n",
    "    \"\"\"\n",
    "    Presents a sequence of normalizing flows as a torch.nn.Module.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, auxiliary, n_flows=1, h_last_dim=None, flow_type=HFlow, flow_flavour=\"hf\"):\n",
    "        super(HouseholderFlow, self).__init__()\n",
    "        self.flow_flavour = flow_flavour\n",
    "        self.v_layers = [[] for _ in range(len(in_features))]\n",
    "        self.n_flows = n_flows\n",
    "        self.flow_type = \"hf\"\n",
    "        flows = []\n",
    "        for i, features in enumerate(reversed(in_features)):\n",
    "            flows += [flow_type().cuda()]\n",
    "            v_layers = [nn.Linear(h_last_dim, features)] + [nn.Linear(features, features) for _ in range(n_flows)]\n",
    "            self.v_layers[i] = nn.ModuleList(v_layers)\n",
    "        if not auxiliary:\n",
    "            self.flows = nn.ModuleList(flows)\n",
    "        else:\n",
    "            self.flows_a = nn.ModuleList(flows)\n",
    "\n",
    "    def forward(self, z, h_last, auxiliary=False):\n",
    "        self.cuda()\n",
    "        v = {}\n",
    "        z = {'0': z, '1': None}\n",
    "        # Householder Flow:\n",
    "        if self.n_flows > 0:\n",
    "            v['1'] = self.v_layers[0][0].cuda()(h_last)\n",
    "            if not auxiliary:\n",
    "                z['1'] = self.flows[0](v['1'], z['0'])\n",
    "            else:\n",
    "                z['1'] = self.flows_a[0](v['1'], z['0'])\n",
    "\n",
    "            for j in range(1, self.n_flows):\n",
    "                v[str(j + 1)] = self.v_layers[0][j].cuda()(v[str(j)])\n",
    "                if not auxiliary:\n",
    "                    z[str(j + 1)] = self.flows[0](v[str(j + 1)], z[str(j)])\n",
    "                else:\n",
    "                    z[str(j + 1)] = self.flows_a[0](v[str(j + 1)], z[str(j)])\n",
    "\n",
    "        return z[str(j + 1)]\n",
    "\n",
    "\n",
    "class ccLinIAF(nn.Module):\n",
    "    def __init__(self, in_features, n_flows=1, h_last_dim=None, flow_flavour=\"ccLinIAF\", auxiliary=False, flow_type=linIAF):\n",
    "        super().__init__()\n",
    "        self.n_combination = n_flows\n",
    "        self.n_flows = n_flows\n",
    "        self.flow_flavour = flow_flavour\n",
    "        flows = []\n",
    "        combination_l = []\n",
    "        encoder_y = []\n",
    "        encoder_L = []\n",
    "\n",
    "        for i, features in enumerate(list(reversed(in_features))):\n",
    "            flows += [flow_type(features).cuda()]\n",
    "            combination_l += [CombinationL(features, self.n_combination)]\n",
    "            encoder_y += [nn.Linear(h_last_dim, self.n_combination)]\n",
    "            encoder_L += [nn.Linear(h_last_dim, (features ** 2) * self.n_combination)]\n",
    "        if not auxiliary:\n",
    "            self.flows = nn.ModuleList(flows)\n",
    "            self.combination_l = nn.ModuleList(combination_l)\n",
    "            self.encoder_y = nn.ModuleList(encoder_y)\n",
    "            self.encoder_L = nn.ModuleList(encoder_L)\n",
    "        else:\n",
    "            self.flows_a = nn.ModuleList(flows)\n",
    "            self.combination_l_a = nn.ModuleList(combination_l)\n",
    "            self.encoder_y_a = nn.ModuleList(encoder_y)\n",
    "            self.encoder_L_a = nn.ModuleList(encoder_L)\n",
    "\n",
    "        self.cuda()\n",
    "\n",
    "    def forward(self, z, h_last, auxiliary=False, k=0):\n",
    "        z = {'0': z, '1': None}\n",
    "        if not auxiliary:\n",
    "            l = self.encoder_L[k](h_last)\n",
    "            y = F.softmax(self.encoder_y[k](h_last), dim=0)\n",
    "            l_combination = self.combination_l[k](l, y)\n",
    "            z['1'] = self.flows[k](l_combination, z['0'])\n",
    "        else:\n",
    "            l = self.encoder_L_a[k](h_last)\n",
    "            y = F.softmax(self.encoder_y_a[k](h_last), dim=0)\n",
    "            l_combination = self.combination_l_a[k](l, y)\n",
    "            z['1'] = self.flows_a[k](l_combination, z['0'])\n",
    "\n",
    "        return z['1']\n",
    "    \n",
    "    \n",
    "    \n",
    "class Stochastic(nn.Module):\n",
    "    \"\"\"\n",
    "    Base stochastic layer that uses the\n",
    "    reparametrization trick [Kingma 2013]\n",
    "    to draw a sample from a distribution\n",
    "    parametrised by mu and log_var.\n",
    "    \"\"\"\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        epsilon = Variable(torch.randn(mu.size()), requires_grad=False)\n",
    "\n",
    "        if mu.is_cuda:\n",
    "            epsilon = epsilon.cuda()\n",
    "\n",
    "        # log_std = 0.5 * log_var\n",
    "        # std = exp(log_std)\n",
    "        std = log_var.mul(0.5).exp_()\n",
    "\n",
    "        # z = std * epsilon + mu\n",
    "        z = mu.addcmul(std, epsilon)\n",
    "\n",
    "        return z\n",
    "\n",
    "\n",
    "class GaussianSample(Stochastic):\n",
    "    \"\"\"\n",
    "    Layer that represents a sample from a\n",
    "    Gaussian distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GaussianSample, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.mu = nn.Linear(in_features, out_features)\n",
    "        self.log_var = nn.Linear(in_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = self.mu(x)\n",
    "        log_var = F.softplus(self.log_var(x))\n",
    "\n",
    "        return self.reparameterize(mu, log_var), mu, log_var\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channel, channel, activation=nn.ReLU, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.linear = nn.Sequential(\n",
    "            activation(),\n",
    "            nn.Linear(in_channel, channel, 3, padding=1),\n",
    "            activation(),\n",
    "            nn.Linear(channel, in_channel, 1),\n",
    "        )\n",
    "        self.conv.apply(random_init)\n",
    "    def forward(self, input):\n",
    "        out = self.conv.to(self.device)(input)\n",
    "        out += input\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResBlockDeconv(nn.Module):\n",
    "    def __init__(self, in_channel, channel, activation=nn.ReLU, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.conv = nn.Sequential(\n",
    "            activation(),\n",
    "            nn.ConvTranspose1d(in_channel, channel, 1),\n",
    "            activation(),\n",
    "            nn.ConvTranspose1d(channel, in_channel, 3, padding=1),\n",
    "        )\n",
    "        self.conv.apply(random_init)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.conv.to(self.device)(input)\n",
    "        out += input\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 z_dim,\n",
    "                 batchnorm,\n",
    "                 activation=torch.nn.GELU,\n",
    "                 flow_type=\"nf\",\n",
    "                 n_flows=2,\n",
    "                 n_res=3,\n",
    "                 gated=True,\n",
    "                 has_dense=True,\n",
    "                 resblocks=False,\n",
    "                 ):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            device = 'cuda'\n",
    "        else:\n",
    "            device = 'cpu'\n",
    "\n",
    "        self.device = device\n",
    "        self.bns = []\n",
    "        self.bns_deconv = []\n",
    "        self.GaussianSample = GaussianSample(z_dim, z_dim).to(device)\n",
    "        self.activation = activation()\n",
    "\n",
    "        self.n_res = n_res\n",
    "\n",
    "        self.has_dense = has_dense\n",
    "        self.batchnorm = batchnorm\n",
    "        self.a_dim = None\n",
    "        self.dense1 = torch.nn.Linear(in_features=772, out_features=z_dim)\n",
    "        self.dense2 = torch.nn.Linear(in_features=z_dim, out_features=772)\n",
    "        self.dense1_bn = nn.BatchNorm1d(num_features=z_dim)\n",
    "        self.dense2_bn = nn.BatchNorm1d(num_features=772)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.bns = nn.ModuleList(self.bns)\n",
    "        self.bns_deconv = nn.ModuleList(self.bns_deconv)\n",
    "        self.flow_type = flow_type\n",
    "        self.n_flows = n_flows\n",
    "        if self.flow_type == \"nf\":\n",
    "            self.flow = NormalizingFlows(in_features=[z_dim], n_flows=n_flows)\n",
    "        if self.flow_type == \"hf\":\n",
    "            self.flow = HouseholderFlow(in_features=[z_dim], auxiliary=False, n_flows=n_flows, h_last_dim=z_dim)\n",
    "        if self.flow_type == \"iaf\":\n",
    "            self.flow = IAF(z_dim, n_flows=n_flows, num_hidden=n_flows, h_size=z_dim, forget_bias=1., conv1d=False)\n",
    "        if self.flow_type == \"ccliniaf\":\n",
    "            self.flow = ccLinIAF(in_features=[z_dim], auxiliary=False, n_flows=n_flows, h_last_dim=z_dim)\n",
    "        if self.flow_type == \"o-sylvester\":\n",
    "            self.flow = SylvesterFlows(in_features=[z_dim], flow_flavour='o-sylvester', n_flows=1, h_last_dim=None)\n",
    "\n",
    "    def random_init(self, init_func=torch.nn.init.kaiming_uniform_):\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv1d) or isinstance(m, nn.ConvTranspose1d):\n",
    "                init_func(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "    def _kld(self, z, q_param, h_last=None, p_param=None):\n",
    "        if len(z.shape) == 1:\n",
    "            z = z.view(1, -1)\n",
    "        if (self.flow_type == \"nf\") and self.n_flows > 0:\n",
    "            (mu, log_var) = q_param\n",
    "            f_z, log_det_z = self.flow(z)\n",
    "            qz = log_gaussian(z, mu, log_var) - sum(log_det_z)\n",
    "            z = f_z\n",
    "        elif (self.flow_type == \"iaf\") and self.n_flows > 0:\n",
    "            (mu, log_var) = q_param\n",
    "            f_z, log_det_z = self.flow(z, h_last)\n",
    "            qz = log_gaussian(z, mu, log_var) - sum(log_det_z)\n",
    "            z = f_z\n",
    "        elif (self.flow_type in ['hf', 'ccliniaf']) and self.n_flows > 0:\n",
    "            (mu, log_var) = q_param\n",
    "            f_z = self.flow(z, h_last)\n",
    "            qz = log_gaussian(z, mu, log_var)\n",
    "            z = f_z\n",
    "        elif self.flow_type in [\"o-sylvester\", \"h-sylvester\", \"t-sylvester\"] and self.n_flows > 0:\n",
    "            mu, log_var, r1, r2, q_ortho, b = q_param\n",
    "            f_z = self.flow(z, r1, r2, q_ortho, b)\n",
    "            qz = log_gaussian(z, mu, log_var)\n",
    "            z = f_z\n",
    "        else:\n",
    "            (mu, log_var) = q_param\n",
    "            qz = log_gaussian(z, mu, log_var)\n",
    "            print(qz)\n",
    "        if p_param is None:\n",
    "            pz = log_standard_gaussian(z)\n",
    "        else:\n",
    "            (mu, log_var) = p_param\n",
    "            pz = log_gaussian(z, mu, log_var)\n",
    "\n",
    "        kl = qz - pz\n",
    "\n",
    "        return kl\n",
    "\n",
    "    def encoder(self, x):\n",
    "        z = self.dense1(x)\n",
    "        z = self.activation(z)\n",
    "        if self.batchnorm:\n",
    "            if z.shape[0] != 1:\n",
    "                z = self.dense1_bn(z)\n",
    "        z = self.dropout(z)\n",
    "        return z\n",
    "\n",
    "    def decoder(self, z):\n",
    "        if self.has_dense:\n",
    "            z = self.dense2(z)\n",
    "            z = self.activation(z)\n",
    "            if self.batchnorm:\n",
    "                if z.shape[0] != 1:\n",
    "                    z = self.dense2_bn(z)\n",
    "            z = self.dropout(z)\n",
    "\n",
    "        x = torch.sigmoid(z)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        z, mu, log_var = self.GaussianSample(x)\n",
    "\n",
    "        # Kullback-Leibler Divergence\n",
    "        kl = self._kld(z, (mu, log_var), x)\n",
    "        print(kl)\n",
    "        if len(z.shape) == 1:\n",
    "            z = z.unsqueeze(0)\n",
    "        rec = self.decoder(z)\n",
    "        return rec, kl\n",
    "\n",
    "    def sample(self, z, y=None):\n",
    "        \"\"\"\n",
    "        Given z ~ N(0, I) generates a sample from\n",
    "        the learned distribution based on p_θ(x|z).\n",
    "        :param z: (torch.autograd.Variable) Random normal variable\n",
    "        :return: (torch.autograd.Variable) generated sample\n",
    "        \"\"\"\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def get_parameters(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(name, param.data.shape)\n",
    "\n",
    "    def get_total_parameters(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(name, param.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('../input/lish-moa/train_features.csv')\n",
    "train_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n",
    "\n",
    "test_features = pd.read_csv('../input/lish-moa/test_features.csv')\n",
    "sample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train_features.columns if col.startswith('c-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENES\n",
    "n_comp = 29\n",
    "\n",
    "# CELLS\n",
    "n_comp = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "l1 = 0.\n",
    "l2 = 0.\n",
    "batch_size = 128\n",
    "mc = 1 # seems to be a problem when mc > 1 for display only, results seem good\n",
    "iw = 1 # seems to be a problem when iw > 1 for display only, results seem good\n",
    "\n",
    "# Neurons layers\n",
    "h_dims = [128, 64]\n",
    "z_dim = 29\n",
    "\n",
    "# number of flows\n",
    "n_combinations = 20 #could be just 1 with number_of_flows?\n",
    "number_of_flows = 8\n",
    "num_elements = 3\n",
    "\n",
    "is_example = True\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_features[GENES]\n",
    "test = test_features[GENES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (GaussianSample): GaussianSample(\n",
       "    (mu): Linear(in_features=29, out_features=29, bias=True)\n",
       "    (log_var): Linear(in_features=29, out_features=29, bias=True)\n",
       "  )\n",
       "  (activation): GELU()\n",
       "  (dense1): Linear(in_features=772, out_features=29, bias=True)\n",
       "  (dense2): Linear(in_features=29, out_features=772, bias=True)\n",
       "  (dense1_bn): BatchNorm1d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dense2_bn): BatchNorm1d(772, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (bns): ModuleList()\n",
       "  (bns_deconv): ModuleList()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Autoencoder(flow_type='vanilla', z_dim=z_dim, batchnorm=16, n_flows=number_of_flows)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "EPOCHS = 20\n",
    "EARLY_STOPPING_STEPS = 11\n",
    "EARLY_STOP = True\n",
    "L1_REG = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-49.1840, -52.0087, -55.9794, -51.7899, -54.1948, -46.8981, -49.7072,\n",
      "        -50.1945, -50.5662, -50.9372, -56.4687, -48.7947, -49.4632, -53.7878,\n",
      "        -54.4238, -53.2916], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "tensor([11.9052, 10.7871, 49.0063,  6.6295, 21.1392, 12.4708, 16.2063, 10.7895,\n",
      "        21.2600, -3.2598, 19.8677, 21.9045,  8.4237, 23.5325, 31.4090, 19.8248],\n",
      "       device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(0.8770, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(17.6185, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function SubBackward0 returned an invalid gradient at index 0 - expected type TensorOptions(dtype=float, device=cpu, layout=Strided, requires_grad=false) but got TensorOptions(dtype=float, device=cuda:0, layout=Strided, requires_grad=false) (validate_outputs at /opt/conda/conda-bld/pytorch_1591914855613/work/torch/csrc/autograd/engine.cpp:484)\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x4e (0x7f43ddc52b5e in /opt/anaconda/lib/python3.7/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x2ae2414 (0x7f440bad9414 in /opt/anaconda/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #2: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x548 (0x7f440badaf48 in /opt/anaconda/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #3: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7f440badced2 in /opt/anaconda/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #4: torch::autograd::Engine::thread_init(int) + 0x39 (0x7f440bad5549 in /opt/anaconda/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #5: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7f440f025638 in /opt/anaconda/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #6: <unknown function> + 0xc819d (0x7f4455fa319d in /opt/anaconda/lib/python3.7/site-packages/zmq/backend/cython/../../../../.././libstdc++.so.6)\nframe #7: <unknown function> + 0x93e9 (0x7f44589d13e9 in /usr/lib/libpthread.so.0)\nframe #8: clone + 0x43 (0x7f44588ff293 in /usr/lib/libc.so.6)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-06ab4599d30b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL1_REG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"EPOCH: {epoch}, train_loss: {train_loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-fe0329d419e4>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(model, optimizer, scheduler, loss_fn, dataloader, device, l1_reg)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m#    regularization_loss += torch.sum(torch.abs(param))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m#loss += l1_reg * regularization_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function SubBackward0 returned an invalid gradient at index 0 - expected type TensorOptions(dtype=float, device=cpu, layout=Strided, requires_grad=false) but got TensorOptions(dtype=float, device=cuda:0, layout=Strided, requires_grad=false) (validate_outputs at /opt/conda/conda-bld/pytorch_1591914855613/work/torch/csrc/autograd/engine.cpp:484)\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x4e (0x7f43ddc52b5e in /opt/anaconda/lib/python3.7/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x2ae2414 (0x7f440bad9414 in /opt/anaconda/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #2: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x548 (0x7f440badaf48 in /opt/anaconda/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #3: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7f440badced2 in /opt/anaconda/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #4: torch::autograd::Engine::thread_init(int) + 0x39 (0x7f440bad5549 in /opt/anaconda/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #5: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7f440f025638 in /opt/anaconda/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #6: <unknown function> + 0xc819d (0x7f4455fa319d in /opt/anaconda/lib/python3.7/site-packages/zmq/backend/cython/../../../../.././libstdc++.so.6)\nframe #7: <unknown function> + 0x93e9 (0x7f44589d13e9 in /usr/lib/libpthread.so.0)\nframe #8: clone + 0x43 (0x7f44588ff293 in /usr/lib/libc.so.6)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MoADataset(train.values)\n",
    "valid_dataset = MoADataset(test.values)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.05, div_factor=1.5e3, \n",
    "                                          max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "early_stopping_steps = EARLY_STOPPING_STEPS\n",
    "early_step = 0\n",
    "\n",
    "best_loss = np.inf\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    train_loss = train_fn(model, optimizer,scheduler, loss_fn, trainloader, DEVICE, L1_REG)\n",
    "    print(f\"EPOCH: {epoch}, train_loss: {train_loss}\")\n",
    "    valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n",
    "    print(f\"EPOCH: {epoch}, valid_loss: {valid_loss}\")\n",
    "\n",
    "    if valid_loss < best_loss:\n",
    "\n",
    "        best_loss = valid_loss\n",
    "        torch.save(model.state_dict(), f\"ae_.pth\")\n",
    "\n",
    "    elif(EARLY_STOP == True):\n",
    "\n",
    "        early_step += 1\n",
    "        if (early_step >= early_stopping_steps):\n",
    "            break\n",
    "\n",
    "\n",
    "x_test = test_[feature_cols].values\n",
    "testdataset = TestDataset(x_test)\n",
    "testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = Model(\n",
    "    num_features=num_features,\n",
    "    num_targets=num_targets,\n",
    "    hidden_size=hidden_size,\n",
    "    n_res=1\n",
    ")\n",
    "prune_model(model)\n",
    "model.load_state_dict(torch.load(f\"ae_.pth\"))\n",
    "model.to(DEVICE)\n",
    "\n",
    "predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n",
    "predictions = inference_fn(model, testloader, DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
